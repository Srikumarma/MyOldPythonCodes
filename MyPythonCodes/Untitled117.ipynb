{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2017905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import sklearn\n",
    "from sklearn.decomposition import NMF\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# **Import Article file which contains all the information about the article the has interacted with**\n",
    "\n",
    "# In[512]:\n",
    "\n",
    "\n",
    "articles_df = pd.read_csv('article.csv')\n",
    "articles_df = articles_df[articles_df['eventType'] == 'CONTENT SHARED']\n",
    "#removing unwanted columns\n",
    "articles_df = articles_df.drop(articles_df.columns[[0, 1, 3,4,5,6,7,8,11,12]], axis=1)\n",
    "articles_df.head()\n",
    "\n",
    "\n",
    "# In[513]:\n",
    "\n",
    "\n",
    "#check for null values\n",
    "articles_df.isnull().values.any()\n",
    "\n",
    "\n",
    "# **Import the user interaction file which will give us the information about how eah user interacted with each articles**\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "df = pd.read_csv('User_Interaction.csv')\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "df = df.drop(df.columns[[0,4,5,6,7]], axis=1)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[514]:\n",
    "\n",
    "\n",
    "# there are about 72312 entries in this dataset\n",
    "df.shape\n",
    "\n",
    "\n",
    "# In[515]:\n",
    "\n",
    "\n",
    "#1895 unique user interactions\n",
    "len(df.personId.unique())\n",
    "\n",
    "\n",
    "# In[516]:\n",
    "\n",
    "\n",
    "# users interacted with 3171 unique artciles\n",
    "len(df.contentId.unique())\n",
    "\n",
    "\n",
    "# In[517]:\n",
    "\n",
    "\n",
    "#The different interations the user had with the artciles\n",
    "df['eventType'].unique()\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "df['eventType']. value_counts(). idxmax()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "df['eventType']. value_counts(). idxmin()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "df['eventType']. value_counts()\n",
    "\n",
    "\n",
    "# **Providing weight to each interaction based on its priority(higher the weight higher the priority)**\n",
    "\n",
    "# In[518]:\n",
    "\n",
    "\n",
    "eventType_weight= {\n",
    "   'VIEW': 1.0,\n",
    "   'LIKE': 2.0, \n",
    "   'BOOKMARK': 2.5, \n",
    "   'FOLLOW': 3.0,\n",
    "   'COMMENT CREATED': 4.0,  \n",
    "}\n",
    "\n",
    "df['eventWeight'] = df['eventType'].apply(lambda x: eventType_weight[x])\n",
    "\n",
    "\n",
    "# In[519]:\n",
    "\n",
    "\n",
    "df.head(10)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "#count of the each article interation by the user\n",
    "count_df_content = df.groupby(['personId', 'contentId']).size()\n",
    "count_df_content\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "#count of how many artciles did the user interact with\n",
    "count_df = df.groupby(['personId', 'contentId']).size().groupby('personId').size()\n",
    "count_df.head(10)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#we only want users with interaction above ceratin threshold to avoid cold start problem\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "#user id with interactions greater than the threshold\n",
    "interaction_threshold_users=count_df[count_df >= 5].reset_index()[['personId']]\n",
    "interaction_threshold_users.head(10)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "#total number of users who have interacted with the articles more than 5 time\n",
    "len(interaction_threshold_users)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#merging the two dataframe, because we use only the datset with users who have interacted more than 5 times\n",
    "df_new=df.merge(interaction_threshold_users)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "df_new.head(10)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "df_new.shape\n",
    "\n",
    "\n",
    "# In[520]:\n",
    "\n",
    "\n",
    "# Using Log transformation, we can fix the skewness of the data\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def smooth_user_preference(x):\n",
    "    return math.log(1+x, 2)\n",
    "    \n",
    "df_log = df_new.groupby(['personId', 'contentId'])['eventWeight'].sum().apply(smooth_user_preference).reset_index()\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "df_log.head(10)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "df_log.shape\n",
    "\n",
    "\n",
    "# In[522]:\n",
    "\n",
    "\n",
    "#spliting the data for training and validation\n",
    "df_train,df_test=train_test_split(df_log,stratify=df_log['personId'],test_size=0.2)\n",
    "print(\"Training data\",len(df_train))\n",
    "print(\"Testing data\",len(df_test))\n",
    "\n",
    "\n",
    "# In[524]:\n",
    "\n",
    "\n",
    "#pivot the dataset for better representation of large data \n",
    "df_pivot=df_train.pivot(index='personId',columns=\"contentId\",values='eventWeight').fillna(0)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "df_pivot.head(20)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "pivot_matrix=df_pivot.values\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "pivot_matrix\n",
    "\n",
    "\n",
    "# In[557]:\n",
    "\n",
    "\n",
    "df_list=list(df_pivot.index)\n",
    "\n",
    "\n",
    "# In[558]:\n",
    "\n",
    "\n",
    "#create a sparse matrix\n",
    "sparse_matrix_df=csr_matrix(pivot_matrix)\n",
    "\n",
    "\n",
    "# In[559]:\n",
    "\n",
    "\n",
    "sparse_matrix_df\n",
    "\n",
    "\n",
    "# In[560]:\n",
    "\n",
    "\n",
    "sparse_matrix_df.toarray()\n",
    "\n",
    "\n",
    "# # Matrix Factorization\n",
    "# \n",
    "# **Three Factorization approaches have been used for this dataset**\n",
    "# 1) Sing Value Decomposition(SVD)\n",
    "# \n",
    "# 2) Nonnegative Matrix Factorization(NMF)\n",
    "# \n",
    "# 3) Stochastic Gradient Decent(SGD)\n",
    "\n",
    "# # SVD\n",
    "\n",
    "# In[530]:\n",
    "\n",
    "\n",
    "#The no. of latent factors have been identified using trail and error\n",
    "\n",
    "No_OF_FACTORS = 45\n",
    "\n",
    "#SVD decomposes a single matrix to 3 smaller matrices.\n",
    "U,sigma,Vt = svds(sparse_matrix_df,k=No_OF_FACTORS)\n",
    "\n",
    "\n",
    "# In[531]:\n",
    "\n",
    "\n",
    "U.shape\n",
    "\n",
    "\n",
    "# In[532]:\n",
    "\n",
    "\n",
    "Vt.shape\n",
    "\n",
    "\n",
    "# In[533]:\n",
    "\n",
    "\n",
    "sigma=np.diag(sigma)\n",
    "sigma.shape\n",
    "\n",
    "\n",
    "# In[534]:\n",
    "\n",
    "\n",
    "#reconstructing the matrix will provide a matrix which is the predicted matrix\n",
    "predicted_df=np.dot(np.dot(U, sigma),Vt)\n",
    "predicted_df.shape\n",
    "\n",
    "\n",
    "# In[243]:\n",
    "\n",
    "\n",
    "#normalization \n",
    "predicted_df_norm = (predicted_df - predicted_df.min()) / (predicted_df.max() - predicted_df.min())\n",
    "\n",
    "\n",
    "# In[244]:\n",
    "\n",
    "\n",
    "predicted_df_norm\n",
    "\n",
    "\n",
    "# In[245]:\n",
    "\n",
    "\n",
    "#matrix to datframe\n",
    "pred_df=pd.DataFrame(predicted_df_norm,columns=df_pivot.columns,index=df_pivot.index).transpose()\n",
    "pred_df\n",
    "\n",
    "\n",
    "# # Recommendation Model for SVD\n",
    "\n",
    "# In[535]:\n",
    "\n",
    "\n",
    "class CFRecommender:\n",
    "    \n",
    "    def __init__(self, cf_predictions_df, items_df=None):\n",
    "        self.cf_predictions_df = cf_predictions_df\n",
    "        self.items_df = items_df\n",
    "        \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=None):\n",
    "        # Get and sort the user's predictions\n",
    "        sorted_user_predictions = self.cf_predictions_df[user_id].sort_values(ascending=False).reset_index().rename(columns={user_id: 'weight'})\n",
    "\n",
    "        recommendations_df = sorted_user_predictions[~sorted_user_predictions['contentId'].isin(items_to_ignore)].sort_values('weight', ascending = False).head(topn)\n",
    "\n",
    "        recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n",
    "                                                        left_on = 'contentId', \n",
    "                                                        right_on = 'contentId')[['weight', 'contentId', 'title','url']]\n",
    "\n",
    "\n",
    "        return recommendations_df\n",
    "    \n",
    "cf_recommender_model = CFRecommender(pred_df, articles_df)\n",
    "\n",
    "\n",
    "# In[537]:\n",
    "\n",
    "\n",
    "#Recommend articles for the provided user using the recommendation system \n",
    "cf_recommender_model.recommend_items(-9016528795238256703, topn=10)\n",
    "\n",
    "\n",
    "# # NMF\n",
    "\n",
    "# In[257]:\n",
    "\n",
    "\n",
    "nmf_model = NMF(n_components=45)\n",
    "\n",
    "\n",
    "# In[265]:\n",
    "\n",
    "\n",
    "nmf_model.fit(sparse_matrix_df)\n",
    "\n",
    "\n",
    "# In[266]:\n",
    "\n",
    "\n",
    "Theta = nmf_model.transform(sparse_matrix_df)       \n",
    "M = nmf_model.components_.T          \n",
    "\n",
    "# Making the predictions\n",
    "NMF_pred = M.dot(Theta.T)              \n",
    "NMF_pred = NMF_pred.T                    \n",
    "\n",
    "\n",
    "# In[267]:\n",
    "\n",
    "\n",
    "NMF_pred\n",
    "\n",
    "\n",
    "# In[268]:\n",
    "\n",
    "\n",
    "NMF_pred_norm=(NMF_pred - NMF_pred.min()) / (NMF_pred.max() - NMF_pred.min())\n",
    "NMF_pred_norm\n",
    "\n",
    "\n",
    "# In[269]:\n",
    "\n",
    "\n",
    "nmf_pred_df=pd.DataFrame(NMF_pred_norm,columns=df_pivot.columns,index=df_pivot.index).transpose()\n",
    "\n",
    "\n",
    "# In[270]:\n",
    "\n",
    "\n",
    "class NMFRecommender:\n",
    "    \n",
    "    def __init__(self, nmf_predictions_df, items_df=None):\n",
    "        self.nmf_predictions_df = nmf_predictions_df\n",
    "        self.items_df = items_df\n",
    "        \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=None):\n",
    "        # Get and sort the user's predictions\n",
    "        sorted_user_predictions = self.nmf_predictions_df[user_id].sort_values(ascending=False).reset_index().rename(columns={user_id: 'weight'})\n",
    "\n",
    "        # Recommend the highest predicted rating movies that the user hasn't seen yet.\n",
    "        recommendations_df = sorted_user_predictions[~sorted_user_predictions['contentId'].isin(items_to_ignore)].sort_values('weight', ascending = False).head(topn)\n",
    "\n",
    "        recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n",
    "                                                        left_on = 'contentId', \n",
    "                                                        right_on = 'contentId')[['weight', 'contentId', 'title','url']]\n",
    "\n",
    "\n",
    "        return recommendations_df\n",
    "    \n",
    "nmf_recommender_model = NMFRecommender(nmf_pred_df, articles_df)\n",
    "\n",
    "\n",
    "# In[271]:\n",
    "\n",
    "\n",
    "nmf_recommender_model.recommend_items(-9016528795238256703, topn=10)\n",
    "\n",
    "\n",
    "# # SGD\n",
    "\n",
    "# In[561]:\n",
    "\n",
    "\n",
    "#check how sparse our data is \n",
    "sparsity = float(len(sparse_matrix_df.nonzero()[0]))\n",
    "sparsity /= (sparse_matrix_df.shape[0] * sparse_matrix_df.shape[1])\n",
    "sparsity *= 100\n",
    "print('{:.2f}%'.format(sparsity))\n",
    "\n",
    "\n",
    "# In[562]:\n",
    "\n",
    "\n",
    "def train_test_split(eventWeight):\n",
    "    train=eventWeight.copy()\n",
    "    \n",
    "    \n",
    "    return train\n",
    "\n",
    "\n",
    "# In[563]:\n",
    "\n",
    "\n",
    "train = train_test_split(sparse_matrix_df)\n",
    "\n",
    "\n",
    "# In[564]:\n",
    "\n",
    "\n",
    "pred_df\n",
    "\n",
    "\n",
    "# In[565]:\n",
    "\n",
    "\n",
    "def predictions(P,Q):\n",
    "    return np.dot(P.T,Q)\n",
    "\n",
    "\n",
    "# In[566]:\n",
    "\n",
    "\n",
    "lmbda = 0.4 # Regularization parameter\n",
    "k = 4 #Factor parameter\n",
    "m, n = train.shape  # Number of users and items\n",
    "\n",
    "n_epochs = 100  # Number of epochs\n",
    "alpha=0.01  # Learning rate\n",
    "\n",
    "P = 3 * np.random.rand(k,m) # Latent user feature matrix\n",
    "Q = 3 * np.random.rand(k,n) # Latent movie feature matrix\n",
    "\n",
    "\n",
    "# In[567]:\n",
    "\n",
    "\n",
    "train.shape\n",
    "\n",
    "\n",
    "# In[568]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Function to find root-mean-square-error\n",
    "def rmse(predictions, ground_truth):\n",
    "    predictions = predictions[ground_truth.nonzero()].flatten()   #Predicted values\n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()  #Original values\n",
    "    return sqrt(mean_squared_error(predictions, ground_truth))\n",
    "\n",
    "\n",
    "# In[569]:\n",
    "\n",
    "\n",
    "users, items = train.nonzero()\n",
    "for u, i in zip(users, items):\n",
    "    error = train[u, i] - predictions(P[:,u],Q[:,i])\n",
    "    P[:, u] += alpha * (error * Q[:, i] - lmbda * P[:, u])\n",
    "    Q[:, i] += alpha * (error * P[:, u] - lmbda * Q[:, i])\n",
    "        \n",
    "\n",
    "\n",
    "# In[570]:\n",
    "\n",
    "\n",
    "SGD_prediction=prediction(P,Q) \n",
    "SGD_prediction\n",
    "\n",
    "\n",
    "# In[571]:\n",
    "\n",
    "\n",
    "SGD_pred_norm=(SGD_prediction - SGD_prediction.min()) / (SGD_prediction.max() - SGD_prediction.min())\n",
    "\n",
    "\n",
    "# In[597]:\n",
    "\n",
    "\n",
    "sgd_pred_df=pd.DataFrame(SGD_pred_norm,columns=pred_df.index,index=pred_df.columns).transpose()\n",
    "\n",
    "\n",
    "# In[598]:\n",
    "\n",
    "\n",
    "class SGDRecommender:\n",
    "    \n",
    "    def __init__(self, sgd_predictions_df, items_df=None):\n",
    "        self.sgd_predictions_df = sgd_predictions_df\n",
    "        self.items_df = items_df\n",
    "        \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=None):\n",
    "        # Get and sort the user's predictions\n",
    "        sorted_user_predictions = self.sgd_predictions_df[user_id].sort_values(ascending=False).reset_index().rename(columns={user_id: 'weight'})\n",
    "\n",
    "        # Recommend the highest predicted rating movies that the user hasn't seen yet.\n",
    "        recommendations_df = sorted_user_predictions[~sorted_user_predictions['contentId'].isin(items_to_ignore)].sort_values('weight', ascending = False).head(topn)\n",
    "\n",
    "        recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n",
    "                                                        left_on = 'contentId', \n",
    "                                                        right_on = 'contentId')[['weight', 'contentId', 'title','url']]\n",
    "\n",
    "\n",
    "        return recommendations_df\n",
    "    \n",
    "sgd_recommender_model = SGDRecommender(sgd_pred_df, articles_df)\n",
    "\n",
    "\n",
    "# In[599]:\n",
    "\n",
    "\n",
    "sgd_recommender_model.recommend_items(-9016528795238256703, topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9507816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopRecommender:\n",
    "    \n",
    "    def __init__(self, cf_predictions_df, items_df):\n",
    "        self.cf_predictions_df = cf_predictions_df\n",
    "        self.items_df = items_df\n",
    "    def recommend_items(self, category_id, items_to_ignore=[], topn=None):\n",
    "        # Get and sort the user's predictions\n",
    "        sorted_user_predictions = self.cf_predictions_df[category_id].sort_values(ascending=False).reset_index().rename(columns={category_id: 'views'})\n",
    "\n",
    "        recommendations_df = sorted_user_predictions[~sorted_user_predictions['video_id'].isin(items_to_ignore)].sort_values('views', ascending = False).head(topn)\n",
    "\n",
    "        recommendations_df = recommendations_df.merge(filtered_df)\n",
    "        \n",
    "        return recommendations_df\n",
    "    \n",
    "pop_recommender_model = PopRecommender(df_pivot,filtered_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
